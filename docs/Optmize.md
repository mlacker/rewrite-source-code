# Optmize

## 排查故障，定位问题的思路

遇到 CPU 占用高，如何分析？

在服务器上执行 top 命令，观察 %CPU 和进程所占用的内存。
如果了解该进程分配的 JVM 堆内存上限，且该进程内存占用近似这个数值，则可以大致推断是 GC 回收的问题。
否则，输入 shift + H 查看线程的 CPU 占用率，或者通过 pidstat 查看，同时可以观察 user%, sys%, wait%
如果 sy% 占用高于 us%，则可以大致推断是线程调度，线程频繁切换用户态和内核态的问题。
如果 us% 占用比例高，则可能是不适当的代码导致死循环，活锁等问题。
如果 wa% 占用高，则可能是 IO 相关的问题，通过 iostat 分析。
如果某个线程的 CPU% 占用过高，记录该线程 id，用 printf 函数转为 16进制，调用 jstack 命令 dump 相应进程的栈快照，通过 grep -A 30 \<tid> 或 less 命令输出栈信息，到代码中定位具体原因。
如果是 GC 问题，通过 jstat 观察该进程的 GC 频率及占用的时间，确认是否为 GC 导致的故障。多次通过 jmap -histo 对比内存活跃对象的分配回收情况定位溢出对象。如果对象直方图不能明显的定位溢出对象，则通过 jmap -dump 命令获取内存快照，通过 MAT 工具分析 shadow 对象分析故障。同时通过增加服务实例、限流等手段降低 gc 压力。

内存溢出根据观察日志异常信息推断溢出类型，如果是堆溢出则在 JVM 参数增加 -XX:+HeapDumpOnOutOfMemoryError

死锁通过 jstack -l 输出死锁信息，或通过线程持有的锁和栈信息分析。

## 优化案例

### 高速率分配，过早提升

不同于常见的内存泄漏和内存溢出导致的 OOM 异常，曾遇到过因大并发场景下，对象分配速率过高，并且过早提升至老年代，导致频繁 GC，进而导致平台请求出现间歇性超时的现象。
